# Paperless Local LLM

KI-gestÃ¼tztes Dokumentenanalyse-System fÃ¼r Paperless-ngx mit Mistral OCR und lokalen Ollama-Modellen.

## Features

- ðŸ” **OCR Processing**: Mistral AI fÃ¼r hochwertige Texterkennung
- ðŸ·ï¸ **Automatische Metadaten**: Titel, Korrespondenten, Tags via LLM
- ðŸ”„ **BestÃ¤tigungs-Loop**: Large Model Analyse â†’ Small Model BestÃ¤tigung â†’ Retry/User-Queue
- ðŸ“Š **Vektor-Suche**: Ã„hnliche Dokumente fÃ¼r Kontext via Qdrant
- ðŸŽ¯ **Tag-basierter Workflow**: UnabhÃ¤ngige Verarbeitungsschritte
- ðŸ–¥ï¸ **Live-Streaming**: LLM-Antworten in Echtzeit im Frontend

## Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vite + React   â”‚â”€â”€â”€â”€â–¶â”‚  FastAPI        â”‚â”€â”€â”€â”€â–¶â”‚  Paperless-ngx  â”‚
â”‚  Frontend       â”‚     â”‚  Backend        â”‚     â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼            â–¼            â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Ollama  â”‚ â”‚ Mistral  â”‚ â”‚  Qdrant  â”‚
              â”‚   LLMs   â”‚ â”‚   OCR    â”‚ â”‚ VectorDB â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### Voraussetzungen

- [Bun](https://bun.sh/) fÃ¼r das Frontend
- [uv](https://github.com/astral-sh/uv) fÃ¼r das Python Backend
- Docker & Docker Compose (optional)
- Laufende Instanzen von:
  - Paperless-ngx
  - Ollama mit deinen bevorzugten Modellen
  - Qdrant (oder via Docker Compose)

### Installation

```bash
# Repository klonen
git clone https://github.com/beastyrabbit/paperless-llm.git
cd paperless-llm

# Alle Dependencies installieren (TurboRepo)
bun install

# Backend Dependencies
cd apps/api/backend
uv sync
```

### Konfiguration

1. Kopiere die Beispiel-Konfiguration:
```bash
cp config.example.yaml config.yaml
```

2. Bearbeite `config.yaml` mit deinen Einstellungen:
```yaml
paperless:
  url: "http://your-paperless-server:8000"
  token: "your-paperless-api-token"

mistral:
  api_key: "your-mistral-api-key"

ollama:
  url: "http://your-ollama-server:11434"
  model_large: "your-large-model"
  model_small: "your-small-model"

qdrant:
  url: "http://your-qdrant-server:6333"
  collection: "paperless-documents"
```

> âš ï¸ **Wichtig**: `config.yaml` ist in `.gitignore` und wird nicht committed. Deine Secrets bleiben lokal.

### Entwicklung

**Option 1 - TurboRepo (alle Apps):**
```bash
bun dev  # Startet Frontend und Backend parallel
```

**Option 2 - Einzeln:**

*Backend:*
```bash
cd apps/api/backend
uv run uvicorn main:app --reload --port 8000
```

*Frontend:*
```bash
cd apps/web
bun dev
```

### Mit Docker Compose

```bash
# Umgebungsvariablen setzen (oder in .env Datei)
export PAPERLESS_URL=http://your-paperless:8000
export PAPERLESS_TOKEN=your-token
export MISTRAL_API_KEY=your-key
export OLLAMA_URL=http://host.docker.internal:11434

# Alle Services starten
docker compose up -d

# Logs anzeigen
docker compose logs -f
```

> ðŸ’¡ **Hinweis**: FÃ¼r den Zugriff auf Ollama auf dem Host-System verwende `host.docker.internal`. Dies funktioniert auf allen Plattformen (Linux, macOS, Windows) dank der `extra_hosts` Konfiguration in docker-compose.yml.

## Workflow

Der Verarbeitungs-Workflow wird Ã¼ber Tags gesteuert:

| Phase | Input-Tag | Output-Tag | Beschreibung |
|-------|-----------|------------|--------------|
| OCR | `llm-pending` | `llm-ocr-done` | Mistral AI OCR |
| Titel | `llm-ocr-done` | `llm-title-done` | Titel generieren |
| Korrespondent | `llm-title-done` | `llm-correspondent-done` | Korrespondent zuweisen |
| Tags | `llm-correspondent-done` | `llm-tags-done` | Tags zuweisen |
| Complete | `llm-tags-done` | `llm-processed` | Fertig |

## API Endpoints

### Settings
- `GET /api/settings` - Aktuelle Einstellungen
- `PATCH /api/settings` - Einstellungen aktualisieren
- `POST /api/settings/test-connection/{service}` - Verbindung testen

### Documents
- `GET /api/documents/queue` - Queue-Statistiken
- `GET /api/documents/pending` - Wartende Dokumente
- `GET /api/documents/{id}` - Dokument-Details

### Processing
- `POST /api/processing/{id}/start` - Verarbeitung starten
- `GET /api/processing/{id}/stream` - SSE-Stream der LLM-Antworten
- `POST /api/processing/{id}/confirm` - Ergebnis bestÃ¤tigen

### Prompts
- `GET /api/prompts` - Alle Prompts auflisten
- `GET /api/prompts/{name}` - Einzelner Prompt

## Projektstruktur

```
paperless_local_llm/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ web/                  # Vite + React Frontend
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ routes/       # Seiten (Dashboard, Documents, etc.)
â”‚   â”‚   â”‚   â”œâ”€â”€ components/   # App-spezifische Komponenten
â”‚   â”‚   â”‚   â”œâ”€â”€ lib/          # API Client, i18n, Utilities
â”‚   â”‚   â”‚   â””â”€â”€ locales/      # Ãœbersetzungen (en.json, de.json)
â”‚   â”‚   â””â”€â”€ public/           # Statische Dateien
â”‚   â”‚
â”‚   â””â”€â”€ api/                  # Python FastAPI Backend
â”‚       â””â”€â”€ backend/
â”‚           â”œâ”€â”€ main.py       # FastAPI App
â”‚           â”œâ”€â”€ config.py     # Konfiguration (liest config.yaml)
â”‚           â”œâ”€â”€ routers/      # API Routes
â”‚           â”œâ”€â”€ services/     # Paperless, Qdrant Clients
â”‚           â”œâ”€â”€ agents/       # LangGraph Agents
â”‚           â”œâ”€â”€ models/       # Pydantic Models
â”‚           â”œâ”€â”€ prompts/      # Prompt Templates
â”‚           â””â”€â”€ worker.py     # Background Worker
â”‚
â”œâ”€â”€ packages/
â”‚   â””â”€â”€ ui/                   # Shared shadcn/ui Komponenten
â”‚       â”œâ”€â”€ src/              # Button, Dialog, Card, etc.
â”‚       â””â”€â”€ lib/utils.ts      # cn() Helper
â”‚
â”œâ”€â”€ turbo.json                # TurboRepo Konfiguration
â”œâ”€â”€ config.example.yaml       # Beispiel-Konfiguration
â”œâ”€â”€ docker-compose.yml        # Docker Setup
â””â”€â”€ README.md
```

## Tech Stack

**Frontend:**
- Vite 7
- React 19
- React Router
- TailwindCSS 4
- shadcn/ui + Hugeicons
- react-i18next

**Backend:**
- Python 3.12
- FastAPI
- LangGraph + LangChain
- Pydantic

**Build System:**
- TurboRepo (Monorepo)
- Bun (Package Manager)

**External:**
- Paperless-ngx
- Ollama (beliebige Modelle)
- Mistral AI (OCR)
- Qdrant Vector DB

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
